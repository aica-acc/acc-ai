import os
import cv2
import torch
import clip
import numpy as np
from datetime import datetime
from PIL import Image

from transformers import BlipProcessor, BlipForConditionalGeneration
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import PydanticOutputParser
from pydantic import ValidationError

from app.domain.cardnews.cardnews_score_model import CardNewsMetrics, CardNewsScore

# GPU ê°•ì œ ë¹„í™œì„±í™” (ì„œë²„ í™˜ê²½ ì•ˆì •ì„± ìœ„í•´)
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"

def cv2_imread_unicode(path: str):
    """
    Windows + í•œê¸€ ê²½ë¡œ ëŒ€ì‘ìš© imread ë˜í¼
    """
    path = str(path)
    data = np.fromfile(path, dtype=np.uint8)
    img = cv2.imdecode(data, cv2.IMREAD_COLOR)
    return img

# ===============================================================
# 1ï¸âƒ£ BLIP Captioning: ì´ë¯¸ì§€ â†’ í…ìŠ¤íŠ¸ ìš”ì•½
# ===============================================================
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

device = "cuda" if torch.cuda.is_available() else "cpu"
clip_model, clip_preprocess = clip.load("ViT-B/32", device=device)

# LLM & Pydantic íŒŒì„œ (ì „ì—­ 1íšŒë§Œ ìƒì„±)
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1)
parser = PydanticOutputParser(pydantic_object=CardNewsMetrics)

# LLMìš© í”„ë¡¬í”„íŠ¸ (ë„¤ê°€ ì œì•ˆí•œ ê³ ë„í™” ë²„ì „ ë°˜ì˜)
cardnews_metrics_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            (
                "ë„ˆëŠ” ì¶•ì œ í™ë³´ìš© ì¹´ë“œë‰´ìŠ¤ë¥¼ í‰ê°€í•˜ëŠ” ì‹œê°ë””ìì¸ ì „ë¬¸ê°€ë‹¤. "
                "ë°°ë„ˆ, í¬ìŠ¤í„°, ì¸ìŠ¤íƒ€ê·¸ë¨ ì¹´ë“œë‰´ìŠ¤ ë“± ë””ì§€í„¸ í™ë³´ë¬¼ì„ ë§ì´ ì ‘í•´ë³¸ ì „ë¬¸ê°€ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•œë‹¤. "
                "ë°˜ë“œì‹œ ì§€ì‹œí•œ JSON í˜•ì‹ë§Œ ë°˜í™˜í•´ì•¼ í•˜ë©°, ì„¤ëª…ì€ ëª¨ë‘ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ì‘ì„±í•œë‹¤."
            ),
        ),
        (
            "user",
            """
ì•„ë˜ëŠ” **ì¹´ë“œë‰´ìŠ¤ ì´ë¯¸ì§€ í•œ ì¥**ì„ ì‚¬ëŒì´ ì„¤ëª…í•œ í…ìŠ¤íŠ¸ë‹¤.
ì´ ì„¤ëª…ì„ ë°”íƒ•ìœ¼ë¡œ ì¹´ë“œë‰´ìŠ¤ì˜ ì‹œê°ì  ì™„ì„±ë„ë¥¼ í‰ê°€í•´ë¼.

### 1. ì ìˆ˜ ë²”ìœ„ì™€ í•´ì„

ê° ì ìˆ˜ëŠ” 0.0 ~ 10.0 ì‚¬ì´ì˜ ì‹¤ìˆ˜(float)ë¡œ í‰ê°€í•œë‹¤.

- 0.0 ~ 3.0  : ë§¤ìš° ë¶€ì¡± â€” ì‹¬ê°í•˜ê²Œ ë¬¸ì œ ìˆëŠ” ìˆ˜ì¤€, ì‹¤ì œ ì„œë¹„ìŠ¤ì— ì“°ê¸° ì–´ë ¤ì›€
- 3.1 ~ 5.0  : ë¶€ì¡± â€” ê°œì„ í•´ì•¼ í•  ì ì´ ë§ìŒ
- 5.1 ~ 7.0  : ë³´í†µ â€” ì‹¤ì‚¬ìš©ì€ ê°€ëŠ¥í•˜ì§€ë§Œ ë‹¤ë“¬ì„ ë¶€ë¶„ì´ ìˆìŒ
- 7.1 ~ 8.5  : ìš°ìˆ˜ â€” ì „ì²´ì ìœ¼ë¡œ ì™„ì„±ë„ê°€ ë†’ê³  ì‹¤ë¬´ì—ì„œë„ ì¶©ë¶„íˆ ì‚¬ìš© ê°€ëŠ¥
- 8.6 ~ 10.0 : ë§¤ìš° ìš°ìˆ˜ â€” ë‹¤ë¥¸ ì‚¬ë¡€ì˜ ë ˆí¼ëŸ°ìŠ¤ë¡œ ì‚¬ìš©í•  ë§Œí•œ ë†’ì€ ì™„ì„±ë„

### 2. í‰ê°€ í•­ëª© ì •ì˜

1) clarity_score / clarity_description  (ì‹œê°ì  ëª…ë£Œë„)
- í•µì‹¬ ì •ë³´(ì¶•ì œëª…, ë‚ ì§œ, ì¥ì†Œ, ì£¼ìš” ë©”ì‹œì§€)ê°€ í•œëˆˆì— ë“¤ì–´ì˜¤ëŠ”ì§€
- ê¸€ì í¬ê¸°, ê³„ì¸µ êµ¬ì¡°(íƒ€ì´í‹€/ì„œë¸Œíƒ€ì´í‹€/ë³¸ë¬¸)ê°€ ëª…í™•í•œì§€
- ë¶ˆí•„ìš”í•˜ê²Œ ë³µì¡í•œ ìš”ì†Œ ë•Œë¬¸ì— ì •ë³´ íŒŒì•…ì´ ë°©í•´ë˜ì§€ ì•ŠëŠ”ì§€

2) contrast_score / contrast_description  (ëª…ë„ ëŒ€ë¹„Â·ê°€ë…ì„±)
- ë°°ê²½ìƒ‰ê³¼ í…ìŠ¤íŠ¸ ìƒ‰ì˜ ëŒ€ë¹„ê°€ ì¶©ë¶„í•œì§€
- ì¤‘ìš”í•œ ì •ë³´ì¼ìˆ˜ë¡ ëŒ€ë¹„ê°€ ê°•í•˜ê²Œ ì²˜ë¦¬ë˜ì–´ ìˆëŠ”ì§€
- í°ìƒ‰/ê²€ì •/í¬ì¸íŠ¸ ì»¬ëŸ¬ ì‚¬ìš©ì´ ê°€ë…ì„±ì„ ë†’ì´ëŠ” ë°©í–¥ìœ¼ë¡œ ì“°ì˜€ëŠ”ì§€

3) distraction_score / distraction_description  (ë°©í•´ìš”ì†ŒÂ·ì‹œì„  ë¶„ì‚° ì •ë„)
- ë¶ˆí•„ìš”í•œ ì•„ì´ì½˜, ì¥ì‹, íŒ¨í„´, ê³¼ë„í•œ ì´í™íŠ¸ê°€ ì‹œì„ ì„ ë¹¼ì•—ì§€ ì•ŠëŠ”ì§€
- ì •ë³´ ì „ë‹¬ê³¼ ìƒê´€ì—†ëŠ” ìš”ì†Œê°€ ë„ˆë¬´ ë§ì§€ ì•Šì€ì§€
- í•˜ë‚˜ì˜ ì£¼ìš” ì‹œì„  íë¦„(focal point)ì´ ìœ ì§€ë˜ëŠ”ì§€

4) color_harmony_score / color_harmony_description  (ìƒ‰ìƒ ì¡°í™”Â·í†µì¼ê°)
- ë©”ì¸ ì»¬ëŸ¬/í¬ì¸íŠ¸ ì»¬ëŸ¬ê°€ ì¼ê´€ë˜ê²Œ ì‚¬ìš©ë˜ëŠ”ì§€
- ë¸Œëœë“œ/ì¶•ì œ ì»¨ì…‰ê³¼ ì–´ìš¸ë¦¬ëŠ” ìƒ‰ ì¡°í•©ì¸ì§€
- ìƒ‰ì´ ë„ˆë¬´ ë§ê±°ë‚˜ íŠ€ì–´ì„œ ì „ì²´ì ìœ¼ë¡œ ì‚°ë§Œí•˜ì§€ëŠ” ì•Šì€ì§€

5) balance_score / balance_description  (ë ˆì´ì•„ì›ƒ ê· í˜•Â·êµ¬ì„±)
- ì¢Œìš°/ìƒí•˜ ë¬´ê²Œ ì¤‘ì‹¬ì´ ì ì ˆí•˜ê²Œ ë¶„ë°°ë˜ì–´ ìˆëŠ”ì§€
- í…ìŠ¤íŠ¸ ë°•ìŠ¤, ì´ë¯¸ì§€, ì•„ì´ì½˜ì˜ ë°°ì¹˜ê°€ ì•ˆì •ì ìœ¼ë¡œ ëŠê»´ì§€ëŠ”ì§€
- ì—¬ë°±(í™”ì´íŠ¸ ìŠ¤í˜ì´ìŠ¤)ì´ ì¶©ë¶„íˆ í™•ë³´ë˜ì–´ ë‹µë‹µí•˜ì§€ ì•Šì€ì§€

6) semantic_fit_score / semantic_fit_description  (ì£¼ì œ ì í•©ë„, ì„ íƒì )
- ì¶•ì œì˜ ì£¼ì œ(ì˜ˆ: ë²šê½ƒ, ìš°ì£¼, ìŒì‹, ê°€ì¡±, ì§€ì—­ íŠ¹ì‚°ë¬¼ ë“±)ì™€ ì‹œê° ìš”ì†Œê°€ ì–¼ë§ˆë‚˜ ì˜ ë§ëŠ”ì§€
- ì´ë¯¸ì§€/ìƒ‰ìƒ/ì•„ì´ì½˜/ì¼ëŸ¬ìŠ¤íŠ¸ê°€ ì¶•ì œì˜ ì„±ê²©ê³¼ íƒ€ê²Ÿ(ê°€ì¡±, MZ, ì§€ì—­ ì£¼ë¯¼ ë“±)ì— ì í•©í•œì§€
- ë‹¨ìˆœíˆ ì˜ˆì˜ê¸°ë§Œ í•œ ê²ƒì´ ì•„ë‹ˆë¼, â€œì´ ì¹´ë“œë‰´ìŠ¤ë¥¼ ë³´ë©´ ì–´ë–¤ ì¶•ì œì¸ì§€â€ê°€ ìì—°ìŠ¤ëŸ½ê²Œ ëŠê»´ì§€ëŠ”ì§€
- **ìë™ ë°°ì¹˜ì—ì„œ ê¸°íšì˜ë„ ì •ë³´ê°€ ì „í˜€ ì—†ëŠ” ê²½ìš°**, ëŒ€ëµì ì¸ ëŠë‚Œë§Œìœ¼ë¡œ í‰ê°€í•˜ë˜,
  íŒë‹¨ì´ ì–´ë ¤ìš°ë©´ 5.0 ê·¼ì²˜ì˜ ì¤‘ë¦½ì ì¸ ì ìˆ˜ë¥¼ ì¤„ ìˆ˜ ìˆë‹¤.

### 3. total_score ê³„ì‚° ê·œì¹™

- total_scoreëŠ” ë‹¤ìŒ í•­ëª©ì„ ë™ì¼ ê°€ì¤‘ì¹˜ë¡œ í‰ê·  ë‚¸ ê°’ìœ¼ë¡œ í•œë‹¤.

  - clarity_score
  - contrast_score
  - (10 - distraction_score)  â†’ ë°©í•´ìš”ì†Œê°€ ì ì„ìˆ˜ë¡ ì¢‹ì€ ì¹´ë“œë‰´ìŠ¤ì´ë¯€ë¡œ,
  - color_harmony_score
  - balance_score
  - semantic_fit_score ê°€ ì¡´ì¬í•œë‹¤ë©´ í¬í•¨, ì—†ë‹¤ë©´ ë‚˜ë¨¸ì§€ í•­ëª©ë§Œìœ¼ë¡œ í‰ê· 

- ì†Œìˆ˜ì  ë‘˜ì§¸ ìë¦¬ì—ì„œ ë°˜ì˜¬ë¦¼í•˜ì—¬ ì†Œìˆ˜ì  ì²«ì§¸ ìë¦¬ê¹Œì§€ ë‚¨ê¸´ë‹¤. (ì˜ˆ: 7.36 â†’ 7.4)

### 4. ì„¤ëª…(description) ì‘ì„± ê·œì¹™

- ê° *_descriptionì€ í•œê¸€ 1~3ë¬¸ì¥ ì •ë„ë¡œ êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•œë‹¤.
- ë‹¨ìˆœíˆ â€œì¢‹ë‹¤/ë‚˜ì˜ë‹¤â€ê°€ ì•„ë‹ˆë¼, **ë ˆì´ì•„ì›ƒÂ·ìƒ‰Â·í…ìŠ¤íŠ¸ ë°°ì¹˜Â·ìš”ì†Œ ì„ íƒ** ë“±
  ì‹œê°ì ì¸ íŠ¹ì„±ì´ ë“œëŸ¬ë‚˜ë„ë¡ ì„œìˆ í•œë‹¤.
- ì˜ˆ)
  - clarity_description ì˜ˆì‹œ:
    - "ì¶•ì œëª…ê³¼ ë‚ ì§œê°€ í™”ë©´ ìƒë‹¨ì— í¬ê²Œ ë°°ì¹˜ë˜ì–´ ì²«ëˆˆì— ë“¤ì–´ì˜¤ë©°, ì„œë¸Œ ì •ë³´ëŠ” ê·¸ ì•„ë˜ì— ë‹¨ê³„ì ìœ¼ë¡œ ì •ë¦¬ë˜ì–´ ìˆë‹¤."
  - color_harmony_description ì˜ˆì‹œ:
    - "ë©”ì¸ ì»¬ëŸ¬ì¸ íŒŒë€ìƒ‰ê³¼ ë³´ì¡° ì»¬ëŸ¬ì¸ ë…¸ë€ìƒ‰ì´ ë°˜ë³µ ì‚¬ìš©ë˜ì–´ í†µì¼ê°ì´ ìˆê³ , ê³¼ë„í•˜ê²Œ íŠ€ëŠ” ìƒ‰ì€ ì—†ë‹¤."

### 5. ì¶œë ¥ í˜•ì‹ (JSON ONLY)

- ë°˜ë“œì‹œ **ìˆœìˆ˜ JSON**ë§Œ ë°˜í™˜í•œë‹¤.
- ì„¤ëª… í…ìŠ¤íŠ¸ë‚˜ í•´ì„¤, ë§ˆí¬ë‹¤ìš´, ì½”ë“œ ë¸”ë¡(````json`)ì„ ì¶”ê°€ë¡œ ì“°ì§€ ì•ŠëŠ”ë‹¤.
- ì•„ë˜ `JSON ìŠ¤í‚¤ë§ˆ ì„¤ëª…`ì„ ì—„ê²©íˆ ë”°ë¥´ë¼.

{format_instructions}

---

ì•„ë˜ëŠ” ì‚¬ëŒì´ ë¬˜ì‚¬í•œ ì¹´ë“œë‰´ìŠ¤ ì´ë¯¸ì§€ ì„¤ëª…ì´ë‹¤. ì´ ì„¤ëª…ë§Œ ë³´ê³  ìœ„ ê¸°ì¤€ì— ë”°ë¼ í‰ê°€í•˜ë¼.

[ì´ë¯¸ì§€ ì„¤ëª… ì‹œì‘]
{caption}
[ì´ë¯¸ì§€ ì„¤ëª… ë]
""",
        ),
    ]
).partial(format_instructions=parser.get_format_instructions())


def generate_caption(image_path: str) -> str:
    """BLIP ê¸°ë°˜ ì´ë¯¸ì§€ ìº¡ì…˜ ìƒì„±"""
    raw_image = Image.open(image_path).convert("RGB")
    inputs = processor(raw_image, return_tensors="pt").to(device)
    out = model.generate(**inputs, max_new_tokens=60)
    caption = processor.decode(out[0], skip_special_tokens=True)
    return caption


# ===============================================================
# 2ï¸âƒ£ LLM ê¸°ë°˜ í‰ê°€ (ê³ ë„í™”ëœ í”„ë¡¬í”„íŠ¸ + Pydantic íŒŒì„œ)
# ===============================================================
def analyze_image_features_llm(image_path: str, caption: str) -> CardNewsMetrics:
    """
    BLIP Caption + LLM ê¸°ë°˜ ì‹œê° í’ˆì§ˆ ì ìˆ˜í™”

    - í”„ë¡¬í”„íŠ¸ëŠ” ì¹´ë“œë‰´ìŠ¤ ì „ë¬¸ ë””ìì´ë„ˆ ê´€ì ì˜ ê¸°ì¤€ì„ ìƒì„¸íˆ í¬í•¨
    - PydanticOutputParserë¥¼ ì‚¬ìš©í•´ CardNewsMetrics ìŠ¤í‚¤ë§ˆì— ë§ì¶° êµ¬ì¡°í™”
    """
    try:
        metrics: CardNewsMetrics = (cardnews_metrics_prompt | llm | parser).invoke(
            {"caption": caption}
        )

        # ì•ˆì „ì„ ìœ„í•´ total_scoreë¥¼ í•œ ë²ˆ ë” ì½”ë“œì—ì„œ ì¬ê³„ì‚°í•´ë„ ë¨
        scores_for_total = [
            metrics.clarity_score,
            metrics.contrast_score,
            10 - metrics.distraction_score,  # ë°©í•´ ìš”ì†ŒëŠ” ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
            metrics.color_harmony_score,
            metrics.balance_score,
        ]

        # semantic_fit_scoreê°€ ìˆì„ ë•Œë§Œ í¬í•¨
        if metrics.semantic_fit_score is not None:
            scores_for_total.append(metrics.semantic_fit_score)

        metrics.total_score = round(float(np.mean(scores_for_total)), 1)
        metrics.create_at = datetime.now()
        return metrics

    except ValidationError as e:
        raise ValueError(f"LLM ì‘ë‹µ ê²€ì¦ ì‹¤íŒ¨: {e}")


# ===============================================================
# 3ï¸âƒ£ CLIP + OpenCV ê¸°ë°˜ í‰ê°€ (ê°ê´€ì  ìˆ˜ì¹˜í˜•)
# ===============================================================
def score_cardnews_image(image_path: str, text_prompt: str | None = None) -> CardNewsScore:
    """CLIP + OpenCV ê¸°ë°˜ ê°ê´€ì  ì‹œê° ì ìˆ˜í™”"""

    img = cv2_imread_unicode(image_path)
    if img is None:
        raise ValueError(f"ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    # === Clarity (ê¸€ì/ìš”ì†Œ ì„ ëª…ë„) ===
    clarity_val = cv2.Laplacian(gray, cv2.CV_64F).var()
    clarity_score = np.clip(clarity_val / 500, 0, 10)
    clarity_description = (
        "í…ìŠ¤íŠ¸ ë° ì£¼ìš” ìš”ì†Œê°€ ì„ ëª…í•˜ê²Œ êµ¬ë¶„ë¨" if clarity_score > 6 else "í…ìŠ¤íŠ¸ì™€ ë°°ê²½ì˜ êµ¬ë¶„ì´ ë‹¤ì†Œ íë¦¼"
    )

    # === Contrast (ëª…ë„ ëŒ€ë¹„) ===
    contrast_val = np.std(gray)
    contrast_score = np.clip(contrast_val / 25, 0, 10)
    contrast_description = (
        "ëª…ì•” ëŒ€ë¹„ê°€ ëšœë ·í•˜ì—¬ ì •ë³´ ì „ë‹¬ì´ ìš©ì´í•¨" if contrast_score > 6 else "ëŒ€ë¹„ê°€ ë‚®ì•„ ì‹œê°ì ìœ¼ë¡œ í‰ë©´ì ì¸ ì¸ìƒ"
    )

    # === Distraction (ì‚°ë§Œí•¨) ===
    p = cv2.calcHist([gray], [0], None, [256], [0, 256]) / gray.size
    entropy = -np.sum(p * np.log2(p + 1e-7))
    distraction_score = np.clip(10 - (entropy / 0.8), 0, 10)
    distraction_description = (
        "ë¶ˆí•„ìš”í•œ ì¥ì‹ ìš”ì†Œê°€ ì ê³  ì‹œì„  ì§‘ì¤‘ì´ ìš©ì´í•¨" if distraction_score > 6 else "ì‹œê°ì  ìš”ì†Œê°€ ì‚°ë§Œí•¨"
    )

    # === Color Harmony (ìƒ‰ìƒ ì¡°í™”) ===
    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
    hist_hue = cv2.calcHist([hsv], [0], None, [50], [0, 180])
    hist_hue = hist_hue / hist_hue.sum()
    color_harmony_score = np.clip(10 - np.std(hist_hue) * 100, 0, 10)
    color_harmony_description = (
        "ìƒ‰ìƒ íŒ”ë ˆíŠ¸ì˜ í†µì¼ì„±ì´ ë†’ìŒ" if color_harmony_score > 6 else "ìƒ‰ìƒ ì¡°í•©ì´ ë‹¤ì†Œ ë¶ˆê· í˜•í•¨"
    )

    # === Balance (ë ˆì´ì•„ì›ƒ ê· í˜•) ===
    h, w = gray.shape
    
    left_weight  = float(np.sum(gray[:, : w // 2]))
    right_weight = float(np.sum(gray[:, w // 2:]))

    denom = left_weight + right_weight + 1e-7
    ratio = abs(left_weight - right_weight) / denom  # 0~1

    balance_score = float(np.clip(10 - ratio * 10, 0, 10))
    balance_description = (
        "ì¢Œìš° ì‹œê°ì  ê· í˜•ì´ ì•ˆì •ì " if balance_score > 6 else "í•œìª½ìœ¼ë¡œ ë¬´ê²Œ ì¤‘ì‹¬ì´ ì¹˜ìš°ì¹¨"
    )

    # === Semantic Fit (ì£¼ì œ ì í•©ë„) ===
    if text_prompt:
        image_pil = Image.open(image_path).convert("RGB")
        image_input = clip_preprocess(image_pil).unsqueeze(0).to(device)
        text_input = clip.tokenize([text_prompt]).to(device)
        with torch.no_grad():
            image_features = clip_model.encode_image(image_input)
            text_features = clip_model.encode_text(text_input)
            similarity = torch.cosine_similarity(image_features, text_features).item()
        semantic_fit_score = np.clip(similarity * 10, 0, 10)
        semantic_fit_description = (
            "ì´ë¯¸ì§€ê°€ ê¸°íš ì˜ë„ì™€ ì˜ ì¼ì¹˜í•¨" if semantic_fit_score > 6 else "ì‹œê° ìš”ì†Œì™€ ê¸°íš ì˜ë„ê°€ ë‹¤ì†Œ ì–´ê¸‹ë‚¨"
        )
    else:
        # ìë™ ë°°ì¹˜(ê¸°íšì˜ë„ ì—†ìŒ)ì—ì„œëŠ” ì¤‘ë¦½ê°’ 5.0 + ì„¤ëª…
        semantic_fit_score = 5.0
        semantic_fit_description = "ê¸°íšì˜ë„ ì •ë³´ê°€ ì—†ì–´ ì£¼ì œ ì í•©ë„ë¥¼ ì¤‘ë¦½ì ìœ¼ë¡œ í‰ê°€í•¨"

    # total_scoreëŠ” hybridì—ì„œ ë‹¤ì‹œ ê³„ì‚°
    return CardNewsScore(
        clarity_score=round(clarity_score, 2),
        clarity_description=clarity_description,
        contrast_score=round(contrast_score, 2),
        contrast_description=contrast_description,
        distraction_score=round(distraction_score, 2),
        distraction_description=distraction_description,
        color_harmony_score=round(color_harmony_score, 2),
        color_harmony_description=color_harmony_description,
        balance_score=round(balance_score, 2),
        balance_description=balance_description,
        semantic_fit_score=round(semantic_fit_score, 2),
        semantic_fit_description=semantic_fit_description,
        total_score=0.0,
        create_at=datetime.now(),
    )


# ===============================================================
# 4ï¸âƒ£ Hybrid í‰ê°€ ê²°í•© (LLM + CLIP)
# ===============================================================
def hybrid_cardnews_score(image_path: str, text_prompt: str | None = None) -> CardNewsMetrics:
    """
    ğŸ¯ CLIP + LLM í•˜ì´ë¸Œë¦¬ë“œ ì¹´ë“œë‰´ìŠ¤ í’ˆì§ˆ ì ìˆ˜í™”

    - text_prompt None â†’ ìë™ë°°ì¹˜ (semantic_fitì„ total_score ê³„ì‚°ì—ì„œ ì œì™¸)
    - text_prompt ì¡´ì¬ â†’ ìƒì„±ë¬¼ í‰ê°€ (semantic_fit í¬í•¨)
    """

    # === Step 1: Caption ìƒì„± ===
    caption = generate_caption(image_path)

    # === Step 2: ë‘ í‰ê°€ ì‹¤í–‰ ===
    llm_metrics = analyze_image_features_llm(image_path, caption)  # CardNewsMetrics
    clip_metrics = score_cardnews_image(image_path, text_prompt=text_prompt)  # CardNewsScore

    # === Step 3: ìˆ˜ì¹˜ í•˜ì´ë¸Œë¦¬ë“œ (ë‹¨ìˆœ í‰ê· ) ===
    def avg(a: float, b: float) -> float:
        return round((a + b) / 2, 2)

    clarity_score = avg(llm_metrics.clarity_score, clip_metrics.clarity_score)
    contrast_score = avg(llm_metrics.contrast_score, clip_metrics.contrast_score)
    distraction_score = avg(llm_metrics.distraction_score, clip_metrics.distraction_score)
    color_harmony_score = avg(llm_metrics.color_harmony_score, clip_metrics.color_harmony_score)
    balance_score = avg(llm_metrics.balance_score, clip_metrics.balance_score)

    if text_prompt:
        semantic_fit_score = avg(llm_metrics.semantic_fit_score, clip_metrics.semantic_fit_score)
        semantic_fit_description = llm_metrics.semantic_fit_description
    else:
        # ìë™ ë°°ì¹˜ì—ì„œëŠ” semantic_fitì€ ê¸°ë¡ë§Œ ë‚¨ê¸°ê³  total_score ê³„ì‚°ì—ì„œëŠ” ì œì™¸
        semantic_fit_score = 0.0
        semantic_fit_description = clip_metrics.semantic_fit_description

    # === Step 4: total_score ì¬ê³„ì‚° ===
    scores_for_total = [
        clarity_score,
        contrast_score,
        10 - distraction_score,  # ë°©í•´ìš”ì†ŒëŠ” ë‚®ì„ìˆ˜ë¡ ì¢‹ìœ¼ë¯€ë¡œ ë’¤ì§‘ì–´ì„œ ì‚¬ìš©
        color_harmony_score,
        balance_score,
    ]
    if text_prompt:
        scores_for_total.append(semantic_fit_score)

    total_score = round(float(np.mean(scores_for_total)), 1)

    # === Step 5: ìµœì¢… CardNewsMetricsë¡œ ë°˜í™˜ (DB cardnews_score ë§¤í•‘ ê¸°ì¤€)
    return CardNewsMetrics(
        clarity_score=clarity_score,
        clarity_description=llm_metrics.clarity_description,
        contrast_score=contrast_score,
        contrast_description=llm_metrics.contrast_description,
        distraction_score=distraction_score,
        distraction_description=llm_metrics.distraction_description,
        color_harmony_score=color_harmony_score,
        color_harmony_description=llm_metrics.color_harmony_description,
        balance_score=balance_score,
        balance_description=llm_metrics.balance_description,
        semantic_fit_score=semantic_fit_score,
        semantic_fit_description=semantic_fit_description,
        total_score=total_score,
        create_at=datetime.now(),
    )
