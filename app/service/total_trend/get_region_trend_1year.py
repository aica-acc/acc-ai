import os
import json
import requests
import datetime
import random
import math
from dotenv import load_dotenv
from openai import OpenAI
from pydantic import BaseModel
from typing import List, Optional

load_dotenv()

NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

client = OpenAI(api_key=OPENAI_API_KEY)

# [ìˆ˜ì • 1] AIì—ê²Œ ìš”ì²­í•  ë°ì´í„° ëª¨ë¸ì—ì„œ 'trend_data' ì œê±°
# (trend_dataëŠ” AIê°€ ì•„ë‹ˆë¼ ìš°ë¦¬ê°€ ê³„ì‚°í•´ì„œ ë„£ì„ ê²ƒì´ë¯€ë¡œ ëºë‹ˆë‹¤)
class KeywordDetail(BaseModel):
    keyword: str
    description: str  
    score: int

class RegionAnalysisResult(BaseModel):
    word_cloud: List[KeywordDetail] 
    family: List[KeywordDetail]
    couple: List[KeywordDetail]
    healing: List[KeywordDetail]
    search_keywords: List[str]

# 1. LLM ë¶„ì„ í•¨ìˆ˜
def analyze_region_with_llm(keyword: str, host_name: str) -> RegionAnalysisResult:
    prompt = f"""
    ì£¼ìµœ ì§€ì—­ '{host_name}'ê³¼ ì¶•ì œ '{keyword}'ë¥¼ ë¶„ì„í•˜ì„¸ìš”.

    [ë¯¸ì…˜ 1: ì›Œë“œí´ë¼ìš°ë“œ - 20ê°œ]
    - '{host_name}'ì™€ '{keyword}'ì˜ ë¶„ìœ„ê¸°, íŠ¹ì‚°ë¬¼, ê°ì„± ë‹¨ì–´ 20ê°œ.
    - ì ìˆ˜(1~10).

    [ë¯¸ì…˜ 2: íƒ€ê¹ƒë³„ ì½”ìŠ¤ (ê° 4ê°œ)]
    - family, couple, healing íƒ€ê¹ƒ ë§ì¶¤ ì¥ì†Œ.

    [ë¯¸ì…˜ 3: ê²€ìƒ‰ í‚¤ì›Œë“œ]
    - ë„¤ì´ë²„ ë°ì´í„°ë© ì¡°íšŒìš© ëŒ€í‘œ í‚¤ì›Œë“œ 5ê°œ.

    JSON í˜•ì‹ì„ ì¤€ìˆ˜í•˜ì„¸ìš”.
    """
    try:
        res = client.chat.completions.parse(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ì§€ì—­ ì¶•ì œ íŠ¸ë Œë“œ ë¶„ì„ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            response_format=RegionAnalysisResult
        )
        return res.choices[0].message.parsed
    except Exception as e:
        print(f"âŒ LLM Error: {e}")
        # ì—ëŸ¬ ì‹œ ë”ë¯¸ ë°ì´í„° ë°˜í™˜
        dummy = KeywordDetail(keyword=f"{host_name}", description="ë¶„ì„ ë°ì´í„° ì—†ìŒ", score=5)
        return RegionAnalysisResult(
            word_cloud=[dummy], family=[dummy], couple=[dummy], healing=[dummy], 
            search_keywords=[f"{host_name} ì—¬í–‰"]
        )

# 2. ë°ì´í„° ë¶€ì¡± ì‹œ 'ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°' ìƒì„± (Fallback)
def generate_fallback_trend(start_date_str: str):
    print("âš ï¸ ë„¤ì´ë²„ ë°ì´í„° ë¶€ì¡± -> ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„± (Fallback Logic ê°€ë™)")
    
    try:
        if start_date_str:
            base_date = datetime.datetime.strptime(start_date_str, "%Y-%m-%d").date()
        else:
            base_date = datetime.date.today()
    except:
        base_date = datetime.date.today()

    weekly_data = []
    # 1ë…„ì¹˜ (52ì£¼) ìƒì„±
    for i in range(-26, 26): 
        current_date = base_date + datetime.timedelta(weeks=i)
        
        # ì¢…ëª¨ì–‘(Gaussian) ê³¡ì„  ìƒì„±
        peak_factor = math.exp(-(i**2) / 10) 
        
        # 1) ì¶•ì œ ê´€ì‹¬ë„ (0~100)
        festival_value = 5 + (peak_factor * 85) + random.randint(-3, 3)
        
        # 2) ì§€ì—­ ê´€ì‹¬ë„ (í•­ìƒ ì–´ëŠì •ë„ ìˆìŒ + ì¶•ì œë•Œ ì•½ê°„ ìƒìŠ¹)
        region_value = 30 + (peak_factor * 15) + random.randint(-5, 5)

        weekly_data.append({
            "period": current_date.strftime("%Y-%m-%d"),
            "festival": round(max(0, festival_value), 1),
            "region": round(max(0, region_value), 1)
        })
    
    return weekly_data

# 3. [ìˆ˜ì • 2] í‚¤ì›Œë“œë³„ ë¯¸ë‹ˆ íŠ¸ë Œë“œ ìƒì„± (Python í•¨ìˆ˜ë¡œ ì²˜ë¦¬)
def generate_keyword_mini_trend(score: int):
    trends = []
    for i in range(7):
        # ì ìˆ˜ê°€ ë†’ì„ìˆ˜ë¡ ê²€ìƒ‰ëŸ‰ë„ ë†’ê²Œ ì‹œë®¬ë ˆì´ì…˜
        val = (score * 8) + random.randint(10, 30)
        if i > 3: val -= random.randint(0, 10)
        trends.append({"day": f"D-{6-i}", "value": val})
    return trends

# 4. ë©”ì¸ í•¨ìˆ˜
def get_region_trend_1year(keyword: str, host_name: str, festival_start_date: str):
    print(f"\n[RegionTrend] ë¶„ì„ ìš”ì²­: {host_name} vs {keyword} (Date: {festival_start_date})")

    # (1) AI ë¶„ì„ ìˆ˜í–‰ (ì—¬ê¸°ì„œëŠ” trend_data ì—†ì´ ìˆœìˆ˜ í…ìŠ¤íŠ¸ ì •ë³´ë§Œ ê°€ì ¸ì˜´)
    ai_result = analyze_region_with_llm(keyword, host_name)
    
    # (2) ë„¤ì´ë²„ ë°ì´í„°ë© ìš”ì²­
    url = "https://openapi.naver.com/v1/datalab/search"
    today = datetime.date.today()
    one_year_ago = today - datetime.timedelta(days=360)

    # ê²€ìƒ‰ì–´ ê·¸ë£¹ ì„¤ì •
    body = {
        "startDate": one_year_ago.strftime("%Y-%m-%d"),
        "endDate": today.strftime("%Y-%m-%d"),
        "timeUnit": "week",
        "keywordGroups": [
            {"groupName": "festival", "keywords": [keyword, f"{keyword} ì¶•ì œ"]},
            {"groupName": "region", "keywords": [f"{host_name} ì—¬í–‰", f"{host_name} ê°€ë³¼ë§Œí•œê³³"]}
        ]
    }

    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET,
        "Content-Type": "application/json; charset=utf-8"
    }

    weekly_data = []
    
    try:
        res = requests.post(url, headers=headers, data=json.dumps(body).encode("utf-8"), timeout=5)
        data = res.json()
        
        # ë°ì´í„° ìœ íš¨ì„± ì²´í¬
        if "results" in data and len(data["results"]) >= 2:
            f_data = data["results"][0]["data"] # festival
            r_data = data["results"][1]["data"] # region
            
            total_f_val = sum([d["ratio"] for d in f_data])
            
            # ê²€ìƒ‰ëŸ‰ì´ ë„ˆë¬´ ì ìœ¼ë©´(ê±°ì˜ 0ì´ë©´) ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ ì „í™˜
            if total_f_val < 5: 
                print("ğŸ“‰ ì‹¤ì œ ê²€ìƒ‰ëŸ‰ ë§¤ìš° ì ìŒ -> ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ì‚¬ìš©")
                raise Exception("Low Data Volume")

            # ë‚ ì§œ ë§¤í•‘ ë³‘í•©
            date_map = {}
            for item in f_data: date_map[item["period"]] = {"festival": item["ratio"], "region": 0}
            for item in r_data:
                if item["period"] in date_map:
                    date_map[item["period"]]["region"] = item["ratio"]
                else:
                    date_map[item["period"]] = {"festival": 0, "region": item["ratio"]}
            
            for date in sorted(date_map.keys()):
                weekly_data.append({
                    "period": date,
                    "festival": date_map[date]["festival"],
                    "region": date_map[date]["region"]
                })
                
        else:
            raise Exception("No Data from Naver")

    except Exception as e:
        # ì—ëŸ¬ ë°œìƒ ì‹œ Fallback ê°€ë™
        weekly_data = generate_fallback_trend(festival_start_date)

    # (3) [ìˆ˜ì • 3] AI ê²°ê³¼ì— 'trend_data' ìˆ˜ë™ ì£¼ì…
    enriched_word_cloud = []
    if ai_result.word_cloud:
        for item in ai_result.word_cloud:
            item_dict = item.model_dump()
            # ì—¬ê¸°ì„œ trend_dataë¥¼ ìƒì„±í•´ì„œ ë„£ì–´ì¤Œ (AIê°€ ì•„ë‹ˆë¼ ì½”ë“œê°€ í•¨)
            item_dict['trend_data'] = generate_keyword_mini_trend(item.score)
            enriched_word_cloud.append(item_dict)

    return {
        "region_weekly": weekly_data, 
        "word_cloud": enriched_word_cloud,
        "family": [k.model_dump() for k in ai_result.family],
        "couple": [k.model_dump() for k in ai_result.couple],
        "healing": [k.model_dump() for k in ai_result.healing]
    }