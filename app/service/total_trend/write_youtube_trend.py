# write_youtube_trend.py
from dotenv import load_dotenv
load_dotenv()

import os
import glob
import ast
import json
import re
from typing import List, TypedDict, Any, Iterable
from langgraph.graph import StateGraph, START, END
from langchain.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
from langchain_openai import ChatOpenAI
from langchain_community.tools import TavilySearchResults
from pydantic import BaseModel, Field
import requests
from serpapi import google_search
# ============================================
# 1) ê²½ë¡œ ì„¤ì • (ì ˆëŒ€ê²½ë¡œ, ì–´ë””ì„œ ì‹¤í–‰í•´ë„ ì•ˆì „)
# ============================================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.abspath(os.path.join(BASE_DIR, "../../data"))
REPORT_DIR = os.path.abspath(os.path.join(BASE_DIR, "../../data/report"))

os.makedirs(REPORT_DIR, exist_ok=True)


# ============================================
# 2) LangGraph State ì •ì˜
# ============================================
class TopState(TypedDict):
    query: str
    information: str
    context: str
    web_query: List[str]
    web_context: List[dict]
    final_result: dict


# ============================================
# 3) LLM ì„¤ì •
# ============================================
llm = ChatOpenAI(model="gpt-4o")


# ============================================
# 4) Node 1 â€” ë¶„ì„ ì…ë ¥ ì²˜ë¦¬
# ============================================
def analyze_input(state: TopState) -> TopState:
    analyze_template = """
    ì•„ë˜ [information]ì€ íŠ¹ì • 'ì¶•ì œ í‚¤ì›Œë“œ(ì˜ˆ: í¬ë¦¬ìŠ¤ë§ˆìŠ¤, ë¹›ì¶•ì œ, ì—¬ë¦„ì¶•ì œ ë“±)'ë¥¼ ê¸°ë°˜ìœ¼ë¡œ 
    YouTube APIë¡œë¶€í„° ìˆ˜ì§‘í•œ í…ìŠ¤íŠ¸ ë°ì´í„°ì…ë‹ˆë‹¤.

    ë‹¹ì‹ ì˜ ì—­í• ì€ **ì¶•ì œ í™ë³´ë¬¼ ì œì‘ì„ ì¤€ë¹„í•˜ëŠ” íŒ€ì„ ìœ„í•œ íŠ¸ë Œë“œ ë¶„ì„ê°€**ì…ë‹ˆë‹¤.
    ì´ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í˜„ì¬ í•œêµ­ì—ì„œ ê´€ì°°ë˜ëŠ” **ì¶•ì œ/ì‹œì¦Œ/ê°ì„±/ì—°ì¶œ íŠ¸ë Œë“œ Top 5**ë¥¼ ë„ì¶œí•˜ì‹œì˜¤.

    [ê·œì¹™]
    - ì™œ ì§€ê¸ˆ ì´ íŠ¸ë Œë“œê°€ ëœ¨ëŠ”ì§€(Why now)
    - Target segment (ëª…ì‹œì  íƒ€ê¹ƒì¸µ)
    - Differentiators (2~3ê°œ)
    - ë¶„ì„ë¬¸ë‹¨ì—ì„œ '(why now)' ê°™ì€ í‘œê¸° ì“°ì§€ ë§ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì‘ì„±

    [information]
    {information}
    """

    prompt = ChatPromptTemplate.from_template(analyze_template)
    chain = prompt | llm | StrOutputParser()

    result = chain.invoke({
        "query": state["query"],
        "information": state["information"]
    })

    return {**state, "context": result}


# ============================================
# 5) Node 2 â€” Web Query ìƒì„±
# ============================================
def make_web_query(state: TopState) -> TopState:
    search_template = """
    ë‹¤ìŒ [context]ì˜ ì£¼ìš” ì´ìŠˆ ë°°ê²½ì„ ê²€ìƒ‰í•˜ê¸° ìœ„í•œ í•œêµ­ì–´ ì¿¼ë¦¬ 5ê°œë¥¼ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì‘ì„±í•˜ì‹œì˜¤.
    ì•„ë˜ [example] í˜•ì‹ì„ ì§€í‚¤ì‹œì˜¤.

    [example]
    [ë„ì‹¬í˜• ì¶•ì œ ì¡°ëª… ì—°ì¶œ íŠ¸ë Œë“œ, ê²¨ìš¸ ì‹œì¦Œ ì•¼ê°„ ì½˜í…ì¸  ì¦ê°€, ì§€ì—­ ì¶•ì œ ê´€ê´‘ê° ë°©ë¬¸ íŒ¨í„´, SNS ê¸°ë°˜ ì¶•ì œ í™ë³´ ì „ëµ, ê³„ì ˆ í…Œë§ˆí˜• í¬í† ì¡´ íŠ¸ë Œë“œ]

    [context]
    {context}
    """

    prompt = ChatPromptTemplate.from_template(search_template)
    chain = prompt | llm | StrOutputParser()

    raw = chain.invoke({"context": state["context"]})

    try:
        queries = ast.literal_eval(raw)
    except:
        queries = [raw]

    return {**state, "web_query": queries}


# ============================================
# 6) Node 3 â€” ì›¹ ê²€ìƒ‰
# ============================================
def web_search(state: TopState) -> TopState:
    tavily = TavilySearchResults(max_results=2)
    results = []

    for q in state["web_query"]:
        res = tavily.invoke(q)
        results.append({
            "query": q,
            "urls": [x["url"] for x in res],
            "context": [x["content"] for x in res],
        })

    return {**state, "web_context": results}


# ============================================
# 7) Node 4 â€” ìµœì¢… íŠ¸ë Œë“œ ê²°ê³¼ ìƒì„± (Structured Output)
# ============================================
class TrendItem(BaseModel):
    trend: str
    subtitle: str
    analysis: str
    recommendations: List[str] = Field(default_factory=list)
    sources: List[str] = Field(default_factory=list)


class TrendOutput(BaseModel):
    items: List[TrendItem]


structured_llm = llm.with_structured_output(TrendOutput)


example_for_final = """
[
  {
    "trend": "í¬ë¦¬ìŠ¤ë§ˆìŠ¤ ê°ì„± ì†Œë¹„ì˜ ì˜ìƒí™” íŠ¸ë Œë“œ",
    "subtitle": "ë¯¸ë””ì–´íŒŒì‚¬ë“œÂ·ìºëŸ´Â·í™ˆë°ì½” ê²°í•©",
    "analysis": "ëª…ë™Â·ë°±í™”ì  ë¯¸ë””ì–´íŒŒì‚¬ë“œ ì¡°ê¸° ì ë“±ìœ¼ë¡œ ì•¼ê°„ ë°©ë¬¸ ì¦ê°€...",
    "recommendations": ["10ì´ˆ ë¦´ìŠ¤ êµ¬ê°„", "ì¡°ëª… ì—°ì¶œ ê°•í™”"],
    "sources": ["https://example.com"]
  }
]
"""


def make_final(state: TopState):
    prompt = ChatPromptTemplate.from_template("""
    ë‹¤ìŒ [context], [web_context] ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ
    ì¶•ì œ íŠ¸ë Œë“œ 5ê°œë¥¼ JSON ë°°ì—´ í˜•íƒœë¡œ ìƒì„±í•˜ë¼.

    [context]
    {context}

    [web_context]
    {web_context}

    [example]
    {example}
    """)

    out: TrendOutput = (prompt | structured_llm).invoke({
        "context": state["context"],
        "web_context": state["web_context"],
        "example": example_for_final
    })

    final = [item.model_dump() for item in out.items]
    return {**state, "final_result": final}


# ============================================
# 8) ê·¸ë˜í”„ ì¡°ë¦½
# ============================================
graph = StateGraph(TopState)
graph.add_node("analyze_input", analyze_input)
graph.add_node("make_web_query", make_web_query)
graph.add_node("web_search", web_search)
graph.add_node("make_final", make_final)

graph.add_edge(START, "analyze_input")
graph.add_edge("analyze_input", "make_web_query")
graph.add_edge("make_web_query", "web_search")
graph.add_edge("web_search", "make_final")
graph.add_edge("make_final", END)

app = graph.compile()


# ============================================
# 9) íŒŒì¼ ì €ì¥ í•¨ìˆ˜
# ============================================
def save_to_file(obj: Any, path: str):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(obj, f, ensure_ascii=False, indent=2)


import os

def serpapi_find_image(trend: str) -> str | None:
    """Google ì´ë¯¸ì§€ ê²€ìƒ‰ìœ¼ë¡œ ì²« ë²ˆì§¸ ê³ í€„ URL ê°€ì ¸ì˜¤ê¸°"""
    try:
        params = {
            "engine": "google_images",
            "q": trend,
            "tbm": "isch",
            "google_domain":"google.co.kr",
            "hl":"ko",
            "gl":"kr",
            "ijn": "0",
            "api_key": os.getenv("SERPAPI_API_KEY")
        }

        search = google_search(params)
        results = search.get_dict()

        # ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        images_results = results.get("images_results")
        if not images_results:
            return None

        # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ URL
        return images_results[0].get("original")
    except Exception as e:
        print("[SerpAPI ì´ë¯¸ì§€ ê²€ìƒ‰ ì˜¤ë¥˜]:", e)
        return None
    


IMAGES_DIR = os.path.join(DATA_DIR, "total_trend_images")
os.makedirs(IMAGES_DIR, exist_ok=True)
HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    )
}

def download_image_fixed(url: str, idx: int) -> str | None:
    if not url:
        return None

    # 1) http â†’ https ê°•ì œ ë³€í™˜
    if url.startswith("http://"):
        url = url.replace("http://", "https://")

    filename = f"image_{idx}.jpg"
    file_path = os.path.join(IMAGES_DIR, filename)

    try:
        # 2) ì²« ë²ˆì§¸ ì‹œë„ (ê¸°ë³¸ ìš”ì²­)
        resp = requests.get(url, headers=HEADERS, timeout=120)
        if resp.status_code == 200:
            with open(file_path, "wb") as f:
                f.write(resp.content)
            return file_path

        print(f"âš ï¸ ê¸°ë³¸ ìš”ì²­ ì‹¤íŒ¨({resp.status_code}) â†’ SSL ë¬´ì‹œ ìš”ì²­ ì¬ì‹œë„")
    except Exception as e:
        print(f"âš ï¸ ê¸°ë³¸ ìš”ì²­ ì˜¤ë¥˜: {e} â†’ SSL ë¬´ì‹œ ì¬ì‹œë„")

    # 3) fallback: SSL ê²€ì¦ ë„ê³  ì¬ì‹œë„
    try:
        resp = requests.get(url, headers=HEADERS, timeout=120, verify=False)
        if resp.status_code == 200:
            with open(file_path, "wb") as f:
                f.write(resp.content)
            return file_path
    except Exception as e:
        print(f"âŒ SSL ë¬´ì‹œ fallbackë„ ì‹¤íŒ¨: {e}")

    return None



# ============================================
# ğŸ”¥ 11) ìµœì¢… ì‹¤í–‰ í•¨ìˆ˜ (ì—¬ê¸°ë§Œ ì™¸ë¶€ì—ì„œ í˜¸ì¶œ)
# ============================================
def run_youtube_trend(keyword: str = "í¬ë¦¬ìŠ¤ë§ˆìŠ¤"):
    """
    YouTube ë°ì´í„° ì½ê¸° â†’ LangGraph ì‹¤í–‰ â†’ ê²°ê³¼ ì €ì¥ â†’ dict ë°˜í™˜
    """
    print("start")
    # 1) ë°ì´í„° ì½ê¸°
    file_list = glob.glob(os.path.join(DATA_DIR, "youtube*"))
    informations = []
    for file in file_list:
        with open(file, "r", encoding="utf-8") as f:
            informations.append(f.read())

    combined_info = "\n".join(informations)

    # 2) ê·¸ë˜í”„ ì‹¤í–‰
    print("start33")
    state = {
        "query": f"{keyword} ê¸°ë°˜ ìœ íŠœë¸Œ íŠ¸ë Œë“œ ë¶„ì„",
        "information": combined_info,
    }

    result = app.invoke(state)
    # í…ŒìŠ¤íŠ¸ ìš© í™•ì¸ ì½”ë“œ ì´ë”° ì§€ìš°ê¸° 
    final = result["final_result"]
    print("== DEBUG result:", result)
    print("== DEBUG result keys:", result.keys())
    print("== DEBUG final:", result.get("final_result"))
    print("== DEBUG final type:", type(result.get("final_result")))
    print("DEBUG final:", final, type(final))

    # 2.5) ì—¬ê¸°ì„œ ì´ë¯¸ì§€ ì²˜ë¦¬ ë¶™ì¸ë‹¤!
    final_with_images = []

    for idx, item in enumerate(final, start=1):
        trend = item["trend"]
        print(f"\nğŸ” íŠ¸ë Œë“œ ì´ë¯¸ì§€ ê²€ìƒ‰: {trend}")

        # SerpAPI ê²€ìƒ‰
        url = serpapi_find_image(trend)
        print(" â†’ SerpAPI URL:", url)

        # ë‹¤ìš´ë¡œë“œí•´ì„œ image_1.jpg ~ image_5.jpg ë¡œ ì €ì¥
        saved_path = download_image_fixed(url, idx)
        print(" â†’ ì €ì¥ë¨:", saved_path)

        # JSON ê²°ê³¼ì—ë„ ë¡œì»¬ ì´ë¯¸ì§€ ê²½ë¡œ ì…ë ¥
        item["image"] = saved_path

        final_with_images.append(item)


      

    # ---------------------------------------------------
    # 3) JSON íŒŒì¼ ì €ì¥ (í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ ë¡œì»¬ê²½ë¡œ í¬í•¨)
    # ---------------------------------------------------
    output_path = os.path.join(REPORT_DIR, "youtube_trend_results.json")
    save_to_file(final_with_images, output_path)

    # ---------------------------------------------------
    # 4) ìµœì¢… ë°˜í™˜
    # ---------------------------------------------------
    return final_with_images

